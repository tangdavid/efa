{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg, stats\n",
    "from sklearn import linear_model\n",
    "from sklearn import decomposition\n",
    "import time\n",
    "import itertools\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Additive Effects and Higher Rank Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tools:\n",
    "    def outer(mat, weights, tensor = False, skip = None):\n",
    "        (m, k) = mat.shape\n",
    "        matType = torch if tensor else np\n",
    "        ret = torch.zeros(m, m) if tensor else np.zeros(shape = (m, m))\n",
    "        \n",
    "        for i in range(k):\n",
    "            for j in range(k):\n",
    "                if i == skip or j == skip:\n",
    "                    continue\n",
    "                    \n",
    "                u = mat[:, i]\n",
    "                v = mat[:, j]\n",
    "                ret += weights[i, j] * matType.outer(u, v)\n",
    "                \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, n, m, k, h2, sbeta, somega):\n",
    "        self.n = n \n",
    "        self.m = m \n",
    "        self.k = k \n",
    "        self.h2 = h2\n",
    "        self.sbeta = sbeta\n",
    "        self.somega = somega\n",
    "\n",
    "        self.simGeno()\n",
    "        self.simEffects()\n",
    "        self.simPheno()\n",
    "\n",
    "    def simGeno(self):\n",
    "        # genotypes are iid binom(2, p) where p normal\n",
    "        # genotypes are scaled and centered\n",
    "        \n",
    "        geno = np.zeros([self.n, self.m])\n",
    "        for i in range(self.m):\n",
    "            p = np.random.beta(2, 2)\n",
    "            snps = np.random.binomial(2, p, self.n)\n",
    "            geno.T[i] = (snps - (2*p))/np.sqrt(2*p*(1-p))\n",
    "        \n",
    "        # interaction effects as khatri rao\n",
    "        inter = linalg.khatri_rao(geno.T, geno.T).T\n",
    "        self.geno, self.inter = geno, inter\n",
    "\n",
    "    def simEffects(self, selfInteraction = False):\n",
    "        m = self.m\n",
    "        k = self.k\n",
    "        \n",
    "        # simulated latent pathways\n",
    "        pathways = np.random.normal(0, 1, m * k).reshape(m, -1) \n",
    "        \n",
    "        # main effects as sum of pathways\n",
    "        beta = np.sum(pathways, axis=1, keepdims=True)\n",
    "                \n",
    "        # currently generate weights from normal (0, 1)\n",
    "        # weights = np.ones(k*k).reshape(k, -1)\n",
    "        weights = np.random.normal(0, 1, k * k).reshape(k, -1) \n",
    "        if selfInteraction: \n",
    "            weights = np.tril(weights, 1) + np.tril(weights, -1).T\n",
    "        else: \n",
    "            weights = np.tril(weights, -1) + np.tril(weights, -1).T\n",
    "        \n",
    "        # simulate interaction matrix by summing over weighted outerproducts\n",
    "        omega = tools.outer(pathways, weights)\n",
    "                \n",
    "        # adding gaussian noise to interaction effects\n",
    "        eomega = np.random.normal(0, self.somega, m * m).reshape(m, -1) \n",
    "        eomega = np.tril(eomega) + np.tril(eomega, -1).T\n",
    "        \n",
    "        # adding gaussian noise to main effects\n",
    "        ebeta = np.random.normal(0, self.sbeta, m).reshape(-1, 1)\n",
    "        \n",
    "        # instance variables\n",
    "        self.pathways = pathways\n",
    "        self.beta = beta + ebeta\n",
    "        self.omegaMat = omega + eomega\n",
    "        self.omega = self.omegaMat.reshape(-1, 1)\n",
    "        self.omegaWeight = weights\n",
    "    \n",
    "    def simPheno(self):\n",
    "        # model with main effects and interactions\n",
    "        mean = self.inter @ self.omega + self.geno @ self.beta\n",
    "        \n",
    "        # add noise to simulate heritability\n",
    "        var = np.var(mean) * (1 - self.h2)/self.h2\n",
    "        sd = np.sqrt(var)\n",
    "        noise = np.random.normal(0, sd, self.n).reshape(-1, 1)\n",
    "        self.pheno = mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decomp:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "    def simData(self, n, m, k = 2, h2 = 0.9, sbeta = 0, somega = 0):\n",
    "        self.data = dataset(n, m, k, h2, sbeta, somega)\n",
    "        self.k = k\n",
    "    \n",
    "    def coordDescent(self):\n",
    "        m = self.data.m\n",
    "        k = self.k\n",
    "        \n",
    "        G = self.data.geno\n",
    "        inter = self.data.inter\n",
    "        Y = self.data.pheno#.reshape(-1, 1)\n",
    "        \n",
    "        thresh = 0.005\n",
    "                \n",
    "        # initialize pathways and weights\n",
    "        weights = np.random.normal(0, 1, k * k).reshape(k, -1) \n",
    "        #weights = np.ones(k * k).reshape(k, -1) \n",
    "        weights = np.tril(weights, -1) + np.tril(weights, -1).T\n",
    "        pathways = np.random.normal(0, 1/10, m * k).reshape(m, k)\n",
    "        iterations = 0\n",
    "        \n",
    "        prevLoss = currentLoss = self.getLoss(pathways, weights)\n",
    "        lossList = [currentLoss]\n",
    "        \n",
    "        while(True):\n",
    "            # iteratively update each pathway\n",
    "            for i in range(k):\n",
    "                # compute constants\n",
    "                mask = [True] * k\n",
    "                mask[i] = False\n",
    "                C1 = tools.outer(pathways, weights, skip=i).reshape(-1, 1)\n",
    "                C1 = inter @ C1\n",
    "                C2 = G @ np.sum(pathways[:,mask], axis=1, keepdims=True) \n",
    "                C = C1 + C2\n",
    "                A = np.sum(G @ pathways @ np.diagflat(weights[i]), axis=1, keepdims=True)\n",
    "                \n",
    "                # equations from the first order conditions\n",
    "                u1 = (4 * G.T @ (A*A*G)) + (4 * G.T @ (A*G)) + (G.T @ G)\n",
    "                u2 = (2 * (A*G).T @ Y) + (G.T @ Y) - (2 * (A*G).T @ C) - (G.T @ C)\n",
    "                u = linalg.inv(u1) @ u2\n",
    "                pathways[:, i] = u.reshape(-1,)\n",
    "                \n",
    "                \n",
    "            # iteratively fit weights\n",
    "            for i in range(k):\n",
    "                for j in range(i):\n",
    "                    ui = pathways[:,i]\n",
    "                    uj = pathways[:,j]\n",
    "                    \n",
    "                    # compute constants\n",
    "                    interacting = ((G @ ui) * (G @ uj)).reshape(-1, 1)\n",
    "                    C1 = tools.outer(pathways, weights) \n",
    "                    C1 = C1 - weights[i, j] * (np.outer(ui, uj) + np.outer(uj, ui))\n",
    "                    C1 = inter @ C1.reshape(-1, 1)\n",
    "                    C2 = G @ np.sum(pathways, axis=1, keepdims=True) \n",
    "                    C = C1 + C2\n",
    "                    \n",
    "                    # equation from the first order conditions\n",
    "                    weight = 1/2 * (interacting.T @ (Y - C))/(interacting.T @ interacting)\n",
    "                    weights[i, j] = weight\n",
    "                    weights[j, i] = weight    \n",
    "            \n",
    "            # monitor convergence\n",
    "            iterations += 1\n",
    "            \n",
    "            prevLoss = currentLoss\n",
    "            currentLoss = self.getLoss(pathways, weights)\n",
    "            lossList.append(currentLoss)\n",
    "            \n",
    "            if np.abs(currentLoss - prevLoss) < thresh: break\n",
    "            if iterations % 10 == 0:\n",
    "                print(\"(iterations,loss):\", iterations, round(currentLoss, 3))\n",
    "            \n",
    "        return (lossList, iterations, pathways, weights)\n",
    "        \n",
    "    def gradDescent(self):\n",
    "        m = self.data.m\n",
    "        k = self.k\n",
    "\n",
    "        G = torch.tensor(self.data.geno, requires_grad = False).double()\n",
    "        inter = torch.tensor(self.data.inter, requires_grad = False).double()\n",
    "        Y = torch.tensor(self.data.pheno, requires_grad = False).double()\n",
    "\n",
    "        thresh = 0.001\n",
    "        \n",
    "        # initialization\n",
    "        #weights = np.ones(k * k).reshape(k, -1) \n",
    "        \n",
    "        weights = np.random.normal(0, 1, k * k).reshape(k, -1) \n",
    "        weights = np.tril(weights, 0) + np.tril(weights, -1).T\n",
    "        weights = torch.tensor(weights, requires_grad = True)\n",
    "\n",
    "        pathways = torch.randn(m, k, requires_grad = True, dtype=torch.float64)\n",
    "        \n",
    "        # use Adam to optimize \n",
    "        optimizer = optim.Adam([weights, pathways])\n",
    "        prevLoss = currentLoss = self.getLoss(pathways.detach().numpy(),\n",
    "                                              weights.detach().numpy())\n",
    "        \n",
    "        lossList = [currentLoss]\n",
    "        iterations = 0\n",
    "        while True:\n",
    "            iterations += 1\n",
    "            # compute effects\n",
    "            interEffect = tools.outer(pathways, weights, tensor = True).view(-1, 1).double()\n",
    "            interEffect = inter @ interEffect\n",
    "            mainEffect = torch.sum(G @ pathways, dim = 1, keepdim = True)\n",
    "            \n",
    "            # compute loss using effects\n",
    "            optimizer.zero_grad()\n",
    "            loss = torch.norm(Y - mainEffect - interEffect)\n",
    "            \n",
    "            # optimize the loss function with gradient descent\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # monitor convergence\n",
    "            prevLoss = currentLoss\n",
    "            currentLoss = loss.item()\n",
    "            lossList.append(currentLoss)\n",
    "            \n",
    "            if np.abs(currentLoss - prevLoss) < thresh: break\n",
    "            if iterations % 1000 == 0: \n",
    "                print(\"(iterations,loss):\", iterations, round(currentLoss, 3))\n",
    "            \n",
    "        pathways = pathways.detach().numpy()\n",
    "        weights = weights.detach().numpy()\n",
    "        return (lossList, iterations, pathways, weights)\n",
    "        \n",
    "    def getLoss(self, pathways, weights):\n",
    "        G = self.data.geno\n",
    "        Y = self.data.pheno\n",
    "        inter = self.data.inter\n",
    "        \n",
    "        interEffect = tools.outer(pathways, weights).reshape(-1, 1)\n",
    "        interEffect = inter @ interEffect\n",
    "        mainEffect = np.sum(G @ pathways, axis=1, keepdims=True)\n",
    "        loss = linalg.norm(Y - mainEffect - interEffect)\n",
    "        return(loss)\n",
    "    \n",
    "    def evalAcc(self):\n",
    "        maxCorr = 0\n",
    "        \n",
    "        groundTruth = self.data.pathways.reshape(-1,)\n",
    "        for perm in itertools.permutations(range(self.k)):\n",
    "            prediction = self.pathways[:,perm].reshape(-1,)\n",
    "            r = stats.pearsonr(prediction, groundTruth)[0]\n",
    "            rsquared = r ** 2\n",
    "            if rsquared > maxCorr: \n",
    "                maxCorr = rsquared\n",
    "        \n",
    "        return(maxCorr)\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(iterations,loss): 10 906.505\n",
      "(iterations,loss): 20 810.021\n",
      "(iterations,loss): 30 765.156\n",
      "(iterations,loss): 40 738.95\n"
     ]
    }
   ],
   "source": [
    "test = decomp()\n",
    "test.simData(100000, 5, k=2, h2=0.999)\n",
    "results = []\n",
    "for i in range(2):\n",
    "    (loss, iterations, pathways, weights) = test.coordDescent()\n",
    "    test.pathways = pathways\n",
    "    results.append((loss[-1], iterations, test.evalAcc()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(231.34016102440654, 36, 0.37434965918154617),\n",
       " (231.33946898785402, 121, 0.3743586281024554)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566.3675404235181"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(iterations,loss): 10 1189.241\n",
      "(iterations,loss): 20 1089.203\n",
      "(iterations,loss): 30 1017.089\n",
      "(iterations,loss): 40 963.17\n",
      "(iterations,loss): 50 921.646\n",
      "(iterations,loss): 60 888.812\n",
      "(iterations,loss): 70 862.268\n",
      "(iterations,loss): 80 840.407\n",
      "(iterations,loss): 90 822.119\n",
      "(iterations,loss): 100 806.612\n",
      "(iterations,loss): 110 793.308\n",
      "(iterations,loss): 120 781.774\n",
      "(iterations,loss): 130 771.68\n",
      "(iterations,loss): 140 762.771\n",
      "(iterations,loss): 150 754.849\n",
      "(iterations,loss): 160 747.754\n",
      "(iterations,loss): 170 741.361\n",
      "(iterations,loss): 180 735.568\n",
      "(iterations,loss): 190 730.292\n",
      "(iterations,loss): 200 725.463\n",
      "(iterations,loss): 210 721.025\n",
      "(iterations,loss): 220 716.931\n",
      "(iterations,loss): 230 713.141\n",
      "(iterations,loss): 240 709.62\n",
      "(iterations,loss): 250 706.34\n",
      "(iterations,loss): 260 703.275\n",
      "(iterations,loss): 270 700.405\n",
      "(iterations,loss): 280 697.71\n",
      "(iterations,loss): 290 695.175\n",
      "(iterations,loss): 300 692.785\n",
      "(iterations,loss): 310 690.527\n",
      "(iterations,loss): 320 688.391\n",
      "(iterations,loss): 330 686.366\n",
      "(iterations,loss): 340 684.444\n",
      "(iterations,loss): 350 682.617\n",
      "(iterations,loss): 360 680.877\n",
      "(iterations,loss): 370 679.218\n",
      "(iterations,loss): 380 677.635\n",
      "(iterations,loss): 390 676.122\n",
      "(iterations,loss): 400 674.675\n",
      "(iterations,loss): 410 673.289\n",
      "(iterations,loss): 420 671.961\n",
      "(iterations,loss): 430 670.686\n",
      "(iterations,loss): 440 669.461\n",
      "(iterations,loss): 450 668.284\n",
      "(iterations,loss): 460 667.151\n",
      "(iterations,loss): 470 666.061\n",
      "(iterations,loss): 480 665.01\n",
      "(iterations,loss): 490 663.997\n",
      "(iterations,loss): 500 663.019\n",
      "(iterations,loss): 510 662.075\n",
      "(iterations,loss): 520 661.162\n",
      "(iterations,loss): 530 660.28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-90b85198a235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecomp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoordDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-48c412acf5a9>\u001b[0m in \u001b[0;36mcoordDescent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mC2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathways\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mC2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mpathways\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagflat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;31m# equations from the first order conditions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = decomp()\n",
    "test.simData(100000, 5, k=2, h2=0.999)\n",
    "test.coordDescent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.46350615, -0.73710432],\n",
       "       [-0.61305411, -1.03978414]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.data.omegaWeight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
